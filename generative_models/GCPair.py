###################################################################
# Code for managing and training a generator/discriminator pair.  #
###################################################################

# basic python
import numpy as np
import numpy.random as npr
from collections import OrderedDict

# theano business
import theano
import theano.tensor as T
import theano.tensor.shared_randomstreams
from theano.sandbox.cuda.rng_curand import CURAND_RandomStreams as RandStream

# phil's sweetness
from NetLayers import HiddenLayer, DiscLayer, softplus_actfun, relu_actfun, \
                      to_fX, constFX
from DKCode import get_adam_updates, get_adadelta_updates


#############################################
# HELPER FUNCTION FOR 1st/2nd ORDER MOMENTS #
#############################################

def projected_moments(X, P, ary_type=None):
    """
    Compute 1st/2nd-order moments after linear transform.

    Return type is always a numpy array. Inputs should both be of the same
    type, which can be either numpy array or theano shared variable.
    """
    assert(not (ary_type is None))
    assert((ary_type == 'theano') or (ary_type == 'numpy'))
    proj_mean = None
    proj_cov = None
    if ary_type == 'theano':
        obs_count = T.cast(X.shape[0], 'floatX')
        Xp = T.dot(X, P)
        Xp_mean = T.mean(Xp, axis=0)
        Xp_centered = Xp - Xp_mean
        Xp_cov = T.dot(Xp_centered.T, Xp_centered) / obs_count
        proj_mean = Xp_mean.eval()
        proj_cov = Xp_cov.eval()
    else:
        Xp = np.dot(X, P)
        Xp_mean = np.mean(Xp, axis=0)
        Xp_centered = Xp - Xp_mean
        Xp_cov = np.dot(Xp_centered.T, Xp_centered) / Xp.shape[0]
        proj_mean = Xp_mean
        proj_cov = Xp_cov
    return [proj_mean, proj_cov]

#############################
# SOME HANDY LOSS FUNCTIONS #
#############################

def logreg_loss(Y, class_sign):
    """
    Simple binomial deviance (i.e. logistic regression) loss.

    This assumes that all predictions in Y have the same target class, which
    is indicated by class_sign, which should be in {-1, +1}. Note: this does
    not "normalize" for the number of predictions in Y.
    """
    loss = T.sum(softplus_actfun(-class_sign * Y))
    return loss

def lsq_loss(Yh, Yt=0.0):
    """
    Least-squares loss for predictions in Yh, given target Yt.
    """
    loss = T.sum((Yh - Yt)**2.0)
    return loss

def hinge_loss(Yh, Yt=0.0):
    """
    Unilateral hinge loss for Yh, given target Yt.
    """
    residual = Yt - Yh
    loss = T.sum((residual * (residual > 0.0)))
    return loss

def ulh_loss(Yh, Yt=0.0, delta=1.0):
    """
    Unilateral Huberized least-squares loss for Yh, given target Yt.
    """
    residual = Yt - Yh
    quad_loss = residual**2.0
    line_loss = (2.0 * delta * abs(residual)) - delta**2.0
    # Construct mask for quadratic loss region
    quad_mask = (abs(residual) < delta) * (residual > 0.0)
    # Construct mask for linear loss region
    line_mask = (abs(residual) >= delta) * (residual > 0.0)
    # Combine the quadratic and linear losses
    loss = T.sum((quad_loss * quad_mask) + (line_loss * line_mask))
    return loss

class GCPair(object):
    """
    Controller for training a generator/discriminator pair.

    The generator must be an instance of the InfNet class implemented in
    "InfNet.py". The discriminator must be an instance of the PeaNet class,
    as implemented in "PeaNet.py".

    Parameters:
        rng: numpy.random.RandomState (for reproducibility)
        Xd: symbolic var for providing samples from the data distribution
        Xp: symbolic var for providing samples from the generator's prior
        d_net: The PeaNet instance that will serve as the discriminator
        g_net: The InfNet instance that will serve as the generator
        obs_dim: dimension of generated data
        z_dim: dimension of latent space (should agree with g_net)
        params: a dict of parameters for controlling various costs
            lam_l2d: regularization on squared discriminator output
            obs_transform: can be 'none' or 'sigmoid'
            mom_mix_rate: rate for updates to the running moment estimates
                          for the distribution generated by g_net
            mom_match_weight: weight for the "moment matching" cost
            mom_match_proj: projection matrix for reduced-dim mom matching
            target_mean: first-order moment to try and match with g_net
            target_cov: second-order moment to try and match with g_net
    """
    def __init__(self, rng=None, Xd=None, Xp=None, d_net=None, g_net=None, \
                 obs_dim=None, z_dim=None, params=None):
        # Do some stuff!
        self.rng = RandStream(rng.randint(100000))
        self.obs_dim = obs_dim
        self.z_dim = z_dim
        self.params = params
        # check that z_dim agrees with input dim for g_net
        assert(self.z_dim == g_net.shared_layers[0].in_dim)
        # set the transform on generator's raw output
        if 'obs_transform' in self.params:
            assert((self.params['obs_transform'] == 'sigmoid') or \
                    (self.params['obs_transform'] == 'none'))
            if self.params['obs_transform'] == 'sigmoid':
                self.obs_transform = lambda x: T.nnet.sigmoid(x)
            else:
                self.obs_transform = lambda x: x
        else:
            self.obs_transform = lambda x: T.nnet.sigmoid(x)

        # symbolic var for inputting samples from the data distribution
        self.Xd = Xd
        # symbolic var for inputting samples from the generator's prior
        self.Xp = Xp
        # symbolic matrix of indices for data inputs
        self.Id = T.lvector(name='gcp_Id')
        # symbolic matrix of indices for noise inputs
        self.In = T.lvector(name='gcp_In')

        # create clones of the given generator and discriminator, after
        # rewiring their computation graphs to take the right inputs
        self.GN = g_net.shared_param_clone(rng=rng, Xd=self.Xp)
        self.out_mean, self.out_logvar, self.out_samples = \
                self.GN.apply(self.Xp, do_samples=True)
        self.Xg = self.obs_transform(self.out_samples)
        self.DN = d_net.shared_param_clone(rng=rng, \
                Xd=T.vertical_stack(self.Xd, self.Xg))

        # shared var learning rate for generator and discriminator
        zero_ary = to_fX( np.zeros((1,)) )
        self.lr_gn = theano.shared(value=zero_ary, name='gcp_lr_gn')
        self.lr_dn = theano.shared(value=zero_ary, name='gcp_lr_dn')
        # shared var momentum parameters for generator and inferencer
        self.mom_1 = theano.shared(value=zero_ary, name='msm_mom_1')
        self.mom_2 = theano.shared(value=zero_ary, name='msm_mom_2')
        self.it_count = theano.shared(value=zero_ary, name='msm_it_count')
        # shared var weights for collaborative classification objective
        self.dw_gn = theano.shared(value=zero_ary, name='gcp_dw_gn')
        self.dw_dn = theano.shared(value=zero_ary, name='gcp_dw_dn')
        # init parameters for controlling learning dynamics
        self.set_sgd_params()    # init SGD rate/momentum
        self.set_disc_weights()  # initcollaborative cost weights for GN/DN
        self.lam_l2d = theano.shared(value=(zero_ary + self.params['lam_l2d']), \
                name='gcp_lam_l2d')

        #######################################################
        # Welcome to: Moment Matching Cost Information Center #
        #######################################################
        #
        # Get parameters for managing the moment matching cost. The moment
        # matching is based on exponentially-decaying estimates of the mean
        # and covariance of the distribution induced by the generator network
        # and the (latent) noise being fed to it.
        #
        # We provide the option of performing moment matching with either the
        # raw generator output, or with linearly-transformed generator output.
        # Either way, the given target mean and covariance should have the
        # appropriate dimension for the space in which we'll be matching the
        # generator's 1st/2nd moments with the target's 1st/2nd moments. For
        # clarity, the computation we'll perform looks like:
        #
        #   Xm = X - np.mean(X, axis=0)
        #   XmP = np.dot(Xm, P)
        #   C = np.dot(XmP.T, XmP)
        #
        # where Xm is the mean-centered samples from the generator and P is
        # the matrix for the linear transform to apply prior to computing
        # the moment matching cost. For simplicity, the above code ignores the
        # use of an exponentially decaying average to track the estimated mean
        # and covariance of the generator's output distribution.
        #
        # The relative contribution of the current batch to these running
        # estimates is determined by self.mom_mix_rate. The mean estimate is
        # first updated based on the current batch, then the current batch
        # is centered with the updated mean, then the covariance estimate is
        # updated with the mean-centered samples in the current batch.
        #
        # Strength of the moment matching cost is given by self.mom_match_cost.
        # Target mean/covariance are given by self.target_mean/self.target_cov.
        # If a linear transform is to be applied prior to matching, it is given
        # by self.mom_match_proj.
        #
        C_init = to_fX( np.zeros((self.obs_dim, self.obs_dim)) )
        m_init = to_fX( np.zeros((self.obs_dim,)) )
        self.dist_cov = theano.shared(C_init, name='gcp_dist_cov')
        self.dist_mean = theano.shared(m_init, name='gcp_dist_mean')
        

        zero_ary = np.zeros((1,))
        mmr = zero_ary + self.params['mom_mix_rate']
        self.mom_mix_rate = theano.shared(name='gcp_mom_mix_rate', \
            value=to_fX(mmr))
        mmw = zero_ary + self.params['mom_match_weight']
        self.mom_match_weight = theano.shared(name='gcp_mom_match_weight', \
            value=to_fX(mmw))
        targ_mean = to_fX( self.params['target_mean'] )
        targ_cov = to_fX( self.params['target_cov'] )
        assert(targ_mean.size == targ_cov.shape[0]) # mean and cov use same dim
        assert(targ_cov.shape[0] == targ_cov.shape[1]) # cov must be square
        self.target_mean = theano.shared(value=targ_mean, name='gcp_target_mean')
        self.target_cov = theano.shared(value=targ_cov, name='gcp_target_cov')
        mmp = np.identity(targ_cov.shape[0]) # default to identity transform
        if 'mom_match_proj' in self.params:
            mmp = self.params['mom_match_proj'] # use a user-specified transform
        assert(mmp.shape[0] == self.obs_dim) # transform matches data dim
        assert(mmp.shape[1] == targ_cov.shape[0]) # and matches mean/cov dims
        mmp = to_fX( mmp )
        self.mom_match_proj = theano.shared(value=mmp, name='gcp_mom_map_proj')
        # finally, we can construct the moment matching cost! and the updates
        # for the running mean/covariance estimates too!
        self.mom_match_cost, self.mom_updates = self._construct_mom_stuff()
        #########################################
        # Thank you for visiting the M.M.C.I.C. #
        #########################################

        # Grab the full set of "optimizable" parameters from the generator
        # and discriminator networks that we'll be working with. We need to
        # ignore parameters in the final layers of the proto-networks in the
        # discriminator network (a generalized pseudo-ensemble). We ignore them
        # because the GCPair requires that they be "bypassed" in favor of some
        # binary classification layers that will be managed by this GCPair.
        self.dn_params = []
        for pn in self.DN.proto_nets:
            for pnl in pn[0:-1]:
                self.dn_params.extend(pnl.params)
        self.gn_params = [p for p in self.GN.mlp_params]
        self.joint_params = self.dn_params + self.gn_params
        # Now construct a binary discriminator layer for each proto-net in the
        # discriminator network. And, add their params to optimization list.
        self._construct_disc_layers(rng)
        self.disc_reg_cost = self.lam_l2d[0] * \
                T.sum([dl.act_l2_sum for dl in self.disc_layers])

        # Construct costs for the generator and discriminator networks based 
        # on collaborative binary classification
        self.disc_cost_dn, self.disc_cost_gn = self._construct_disc_costs()

        # compute small l2 penalty on params
        self.dn_l2_cost = constFX(1e-4) * T.sum([T.sum(p**2.0) for p in self.dn_params])
        self.gn_l2_cost = constFX(1e-4) * T.sum([T.sum(p**2.0) for p in self.gn_params])

        # Cost w.r.t. discriminator parameters is only the collaborative binary
        # classification cost. Cost w.r.t. comprises a collaborative binary
        # classification cost and the (weighted) moment matching cost.
        self.dn_cost = self.disc_cost_dn + self.disc_reg_cost + self.dn_l2_cost
        self.gn_cost = self.disc_cost_gn + self.mom_match_cost + self.gn_l2_cost
        self.joint_cost = self.dn_cost + self.gn_cost

        # Compute gradients on generator and dicriminator parameters
        print("Computing gradients on generator...")
        self.gn_grads = OrderedDict()
        grad_list = T.grad(self.gn_cost, self.gn_params)
        for i, p in enumerate(self.gn_params):
            self.gn_grads[p] = grad_list[i]
        print("Computing gradients on discriminator...")
        self.dn_grads = OrderedDict()
        grad_list = T.grad(self.dn_cost, self.dn_params)
        for i, p in enumerate(self.dn_params):
            self.dn_grads[p] = grad_list[i]

        # Construct the updates for the generator and discriminator network
        self.joint_updates = OrderedDict()
        self.dn_updates = OrderedDict()
        self.gn_updates = OrderedDict()
        for var in self.mom_updates:
            # these updates are for the generator distribution's running first
            # and second-order moment estimates
            self.gn_updates[var] = self.mom_updates[var]
            self.joint_updates[var] = self.gn_updates[var]
        # Construct the updates for the generator and inferencer networks
        self.dn_updates = get_adam_updates(params=self.dn_params, \
                grads=self.dn_grads, alpha=self.lr_dn, \
                beta1=self.mom_1, beta2=self.mom_2, it_count=self.it_count, \
                mom2_init=1e-3, smoothing=1e-8, max_grad_norm=10.0)
        self.gn_updates = get_adam_updates(params=self.gn_params, \
                grads=self.gn_grads, alpha=self.lr_gn, \
                beta1=self.mom_1, beta2=self.mom_2, it_count=self.it_count, \
                mom2_init=1e-3, smoothing=1e-8, max_grad_norm=10.0)
        for k in self.dn_updates:
            self.joint_updates[k] = self.dn_updates[k]
        for k in self.gn_updates:
            self.joint_updates[k] = self.gn_updates[k]

        # Construct batch-based training functions for the generator and
        # discriminator networks, as well as a joint training function.
        print("Compiling generator training function...")
        self.train_gn = self._construct_train_gn()
        print("Compiling discriminator training function...")
        self.train_dn = self._construct_train_dn()
        print("Compiling joint training function...")
        self.train_joint = self._construct_train_joint()

        # Construct a function for computing the ouputs of the generator
        # network for a batch of noise. Presumably, the noise will be drawn
        # from the same distribution that was used in training....
        self.sample_from_gn = self._construct_model_sampler()
        return

    def set_disc_weights(self, dweight_gn=1.0, dweight_dn=1.0):
        """
        Set weights for the collaborative classification cost.
        """
        zero_ary = to_fX( np.zeros((1,)) )
        new_dw_dn = zero_ary + dweight_dn
        self.dw_dn.set_value(new_dw_dn)
        new_dw_gn = zero_ary + dweight_gn
        self.dw_gn.set_value(new_dw_gn)
        return

    def set_sgd_params(self, lr_gn=0.01, lr_dn=0.01, \
                mom_1=0.9, mom_2=0.999):
        """
        Set learning rate and momentum parameter for all updates.
        """
        zero_ary = np.zeros((1,))
        # set learning rates
        new_lr_gn = zero_ary + lr_gn
        self.lr_gn.set_value(to_fX(new_lr_gn))
        new_lr_dn = zero_ary + lr_dn
        self.lr_dn.set_value(to_fX(new_lr_dn))
        # set momentums
        new_mom_1 = zero_ary + mom_1
        self.mom_1.set_value(to_fX(new_mom_1))
        new_mom_2 = zero_ary + mom_2
        self.mom_2.set_value(to_fX(new_mom_2))
        return

    def init_moments(self, sample_count):
        """
        Initialize estimates of the generator distribution's 1st and 2nd-order
        moments based on some large sample of input noise to the generator
        network. Estimates will be performed, and subsequently tracked, in a
        transformed space based on self.mom_match_proj.
        """
        # Compute outputs for the input latent noise in X_noise
        X = self.sample_from_gn(sample_count)
        # Get the transform to apply prior to moment matching
        P = self.mom_match_proj.get_value(borrow=False)
        # Compute post-transform mean and covariance of the outputs
        mu, sigma = projected_moments(X, P, ary_type='numpy')
        # Initialize the generator network's running moment estimates 
        self.dist_cov.set_value(to_fX(sigma))
        self.dist_mean.set_value(to_fX(mu))
        return

    def _construct_disc_layers(self, rng):
        """
        Construct binary discrimination layers for each spawn-net in the
        underlying discrimnator pseudo-ensemble. All spawn-nets spawned from
        the same proto-net will use the same disc-layer parameters.
        """
        self.disc_layers = []
        for sn in self.DN.spawn_nets:
            sn_fl = sn[-1]
            self.disc_layers.append(DiscLayer(rng=rng, \
                    input=sn_fl.noisy_input, in_dim=sn_fl.in_dim))
            self.dn_params.extend(self.disc_layers[-1].params)
        return

    def _construct_disc_costs(self):
        """
        Construct the generator and discriminator collaborative costs.
        """
        gn_costs = []
        dn_costs = []
        for d_layer in self.disc_layers:
            dl_output = d_layer.linear_output
            data_preds = dl_output.take(self.Id, axis=0)
            noise_preds = dl_output.take(self.In, axis=0)
            # Compute dn cost based on predictions for both data and noise
            dn_pred_count = self.Id.size + self.In.size
            dnl_dn_cost = (logreg_loss(data_preds, 1.0) + \
                    logreg_loss(noise_preds, -1.0)) / dn_pred_count
            # Compute gn cost based only on predictions for noise
            gn_pred_count = self.In.size
            #dnl_gn_cost = hinge_loss(noise_preds, 0.0) / gn_pred_count
            dnl_gn_cost = ulh_loss(noise_preds, Yt=0.0, delta=0.5) / gn_pred_count
            dn_costs.append(dnl_dn_cost)
            gn_costs.append(dnl_gn_cost)
        dn_cost = self.dw_dn[0] * T.sum(dn_costs)
        gn_cost = self.dw_gn[0] * T.sum(gn_costs)
        return [dn_cost, gn_cost]

    def _construct_mom_stuff(self):
        """
        Construct the cost function for the moment-matching "regularizer".
        """
        a = self.mom_mix_rate
        batch_size = T.cast(self.Xg.shape[0], 'floatX')
        # Get the generated sample observations for this batch, transformed
        # linearly into the desired space for moment matching...
        X_b = T.dot(self.Xg, self.mom_match_proj)
        # Get the updated generator distribution mean
        batch_mean = T.mean(X_b, axis=0)
        new_mean = ((1.0 - a[0]) * self.dist_mean) + (a[0] * batch_mean)
        # Use the mean to get the updated generator distribution covariance
        X_b_minus_mean = X_b - new_mean
        # Whelp, I guess this line needs the cast... for some reason...
        batch_cov = T.dot(X_b_minus_mean.T, X_b_minus_mean) / batch_size
        new_cov = ((1.0 - a[0]) * self.dist_cov) + (a[0] * batch_cov)
        # Get the cost for deviation from the target distribution's moments
        mean_err = new_mean - self.target_mean
        cov_err = (new_cov - self.target_cov)
        mm_cost = self.mom_match_weight[0] * \
                (T.sum(mean_err**2.0) + T.sum(cov_err**2.0))
        # Construct the updates for the running estimates of the generator
        # distribution's first and second-order moments.
        mom_updates = OrderedDict()
        mom_updates[self.dist_mean] = new_mean
        mom_updates[self.dist_cov] = new_cov
        return [mm_cost, mom_updates]

    def _construct_model_sampler(self):
        """
        Construct a function to train from the underlying generator.
        """
        z_sym = T.matrix()
        xg, _ = self.GN.apply(z_sym, do_samples=False)
        sample_func = theano.function(inputs=[z_sym], outputs=xg)
        def model_sampler(samp_count):
            z_samps = to_fX( npr.randn(samp_count, self.z_dim) )
            model_samps = sample_func(z_samps)
            return model_samps
        return model_sampler



    def _construct_train_gn(self):
        """
        Construct theano function to train generator on its own.
        """
        outputs = [self.mom_match_cost, self.disc_cost_gn, self.disc_cost_dn]
        func = theano.function(inputs=[ self.Xd, self.Xp, self.Id, self.In ], \
                outputs=outputs, \
                updates=self.gn_updates)
        return func

    def _construct_train_dn(self):
        """
        Construct theano function to train discriminator on its own.
        """
        outputs = [self.mom_match_cost, self.disc_cost_gn, self.disc_cost_dn]
        func = theano.function(inputs=[ self.Xd, self.Xp, self.Id, self.In ], \
                outputs=outputs, \
                updates=self.dn_updates)
        return func

    def _construct_train_joint(self):
        """
        Construct theano function to train generator and discriminator jointly.
        """
        outputs = [self.mom_match_cost, self.disc_cost_gn, self.disc_cost_dn]
        func = theano.function(inputs=[ self.Xd, self.Xp, self.Id, self.In ], \
                outputs=outputs, \
                updates=self.joint_updates)
        return func

if __name__=="__main__":
    from load_data import load_udm, load_udm_ss
    from InfNet import InfNet
    from PeaNet import PeaNet
    import utils as utils
    # Simple test code, to check that everything is basically functional.
    print("TESTING...")

    # Initialize a source of randomness
    rng = np.random.RandomState(1234)

    # Load some data to train/validate/test with
    dataset = 'data/mnist.pkl.gz'
    datasets = load_udm(dataset, zero_mean=False)
    Xtr = datasets[0][0]
    tr_samples = Xtr.get_value(borrow=True).shape[0]
    obs_dim = Xtr.get_value(borrow=True).shape[1]
    mm_proj_dim = 250
    z_dim = 200

    # Symbolic input matrix to generator network
    Xp_sym = T.matrix(name='Xp_sym')
    Xd_sym = T.matrix(name='Xd_sym')

    # Do moment matching in some transformed space
    #P = np.identity(obs_dim)
    P = npr.randn(obs_dim, mm_proj_dim) / np.sqrt(float(mm_proj_dim))
    P = theano.shared(value=to_fX(P), name='P_proj')

    target_mean, target_cov = projected_moments(Xtr, P, ary_type='theano')
    P = to_fX(P.get_value(borrow=False))

    ###########################
    # Setup generator network #
    ###########################
    params = {}
    shared_config = [z_dim, 1000, 1000]
    top_config = [shared_config[-1], obs_dim]
    params['shared_config'] = shared_config
    params['mu_config'] = top_config
    params['sigma_config'] = top_config
    params['activation'] = relu_actfun
    params['init_scale'] = 1.5
    params['lam_l2a'] = 0.0
    params['vis_drop'] = 0.0
    params['hid_drop'] = 0.0
    params['bias_noise'] = 0.1
    params['input_noise'] = 0.0
    params['build_theano_funcs'] = False
    GN = InfNet(rng=rng, Xd=Xp_sym, \
            params=params, shared_param_dicts=None)

    ###############################
    # Setup discriminator network #
    ###############################
    dn_params = {}
    # Set up some proto-networks
    pc0 = [28*28, (250, 4), (250, 4), 11]
    dn_params['proto_configs'] = [pc0]
    # Set up some spawn networks
    sc0 = {'proto_key': 0, 'input_noise': 0.1, 'bias_noise': 0.1, 'do_dropout': True}
    #sc1 = {'proto_key': 0, 'input_noise': 0.1, 'bias_noise': 0.1, 'do_dropout': True}
    dn_params['spawn_configs'] = [sc0]
    dn_params['spawn_weights'] = [1.0]
    # Set remaining params
    dn_params['lam_l2a'] = 1e-3
    dn_params['vis_drop'] = 0.2
    dn_params['hid_drop'] = 0.5
    dn_params['init_scale'] = 0.5
    DN = PeaNet(rng=rng, Xd=Xd_sym, params=dn_params)

    ########################################################################
    # Initialize the joint controller for the generator/discriminator pair #
    ########################################################################
    gcp_params = {}
    gcp_params['lam_l2d'] = 1e-2
    gcp_params['mom_mix_rate'] = 0.05
    gcp_params['mom_match_weight'] = 0.05
    gcp_params['mom_match_proj'] = P
    gcp_params['target_mean'] = target_mean
    gcp_params['target_cov'] = target_cov
    GCP = GCPair(rng=rng, Xd=Xd_sym, Xp=Xp_sym, d_net=DN, g_net=GN, \
            z_dim=z_dim, obs_dim=28*28, params=gcp_params)

    # setup some learning parameters
    gn_learn_rate = 0.02
    dn_learn_rate = 0.01
    GCP.set_sgd_params(lr_gn=gn_learn_rate, lr_dn=dn_learn_rate, \
            mom_1=0.8, mom_2=0.98)
    # Init generator's mean and covariance estimates with many samples
    GCP.init_moments(10000)

    batch_idx = T.lvector('batch_idx')
    batch_sample = theano.function(inputs=[ batch_idx ], \
            outputs=Xtr.take(batch_idx, axis=0))

    for i in range(750000):
        tr_idx = npr.randint(low=0,high=tr_samples,size=(100,)).astype(np.int32)
        Xd_batch = batch_sample(tr_idx)
        Xd_batch =to_fX(Xd_batch)
        Xn_np = npr.randn(100, z_dim)
        Xn_batch = to_fX(Xn_np)
        all_idx = np.arange(200)
        data_idx = all_idx[:100]
        noise_idx = all_idx[100:]
        scale = min(1.0, float(i+1)/10000.0)
        GCP.set_disc_weights(dweight_gn=scale, dweight_dn=scale)
        outputs = GCP.train_joint(Xd_batch, Xn_batch, data_idx, noise_idx)
        mom_match_cost = 1.0 * outputs[0]
        disc_cost_gn = 1.0 * outputs[1]
        disc_cost_dn = 1.0 * outputs[2]
        if ((i+1 % 100000) == 0):
            gn_learn_rate = gn_learn_rate * 0.7
            dn_learn_rate = dn_learn_rate * 0.7
            GCP.set_sgd_params(lr_gn=gn_learn_rate, lr_dn=dn_learn_rate, \
                    mom_1=0.8, mom_2=0.98)
        if ((i % 1000) == 0):
            print("batch: {0:d}, mom_match_cost: {1:.4f}, disc_cost_gn: {2:.4f}, disc_cost_dn: {3:.4f}".format( \
                    i, mom_match_cost, disc_cost_gn, disc_cost_dn))
        if ((i % 5000) == 0):
            file_name = "GCP_SAMPLES_b{0:d}.png".format(i)
            Xs = GCP.sample_from_gn(200)
            utils.visualize_samples(Xs, file_name)
            file_name = "GCP_WEIGHTS_b{0:d}.png".format(i)
            utils.visualize(GCP.DN, 0, 0, file_name)
    print("TESTING COMPLETE!")




##############
# EYE BUFFER #
##############
