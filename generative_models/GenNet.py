##################################################################
# Code for networks and whatnot to use in variationalish stuff.  #
##################################################################

# basic python
import numpy as np
import numpy.random as npr
import cPickle
from collections import OrderedDict

# theano business
import theano
import theano.tensor as T
#from theano.tensor.shared_randomstreams import RandomStreams as RandStream
from theano.sandbox.cuda.rng_curand import CURAND_RandomStreams as RandStream

# phil's sweetness
from NetLayers import HiddenLayer, DiscLayer, relu_actfun, \
                      max_normalize
from LogPDFs import log_prob_bernoulli, log_prob_gaussian2

#####################################
# GENERATIVE NETWORK IMPLEMENTATION #
#####################################

def row_normalize(x):
    """
    Normalize rows of matrix x to unit (L2) norm.
    """
    x_normed = x / T.sqrt(T.sum(x**2.,axis=1,keepdims=1)+1e-8)
    return x_normed

def soft_abs(x, smoothing=1e-5):
    """
    Soft absolute value function applied to x.
    """
    sa_x = T.sqrt(x**2. + smoothing)
    return sa_x

class GenNet(object):
    """
    A net that transforms a simple distribution so that it matches some
    more complicated distribution, for some definition of match....

    Parameters:
        rng: a numpy.random RandomState object
        Xp: symbolic matrix for inputting latent variable samples
        prior_sigma: standard deviation of isotropic Gaussian prior that this
                     generator will transform to match some other distribution
        params: a dict of parameters describing the desired network:
            lam_l2a: L2 regularization weight on neuron activations
            vis_drop: drop rate to use on the latent variable space
            hid_drop: drop rate to use on the hidden layer activations
                -- note: vis_drop/hid_drop are optional, with defaults 0.0/0.0
            bias_noise: standard dev for noise on the biases of hidden layers
            mlp_config: list of "layer descriptions"
            out_type: set this to "bernoulli" for generating outputs to match
                      bernoulli-valued observations and set it to "gaussian" to
                      match general real-valued observations.
            decoder: a decoder to apply to outputs generated by this net. this
                     may be used to permit, e.g., PCA as an encoder/decoder for
                     preprocessing.
            mean_transform: an optional transform, indicated by a string, to
                            apply to output gaussian's mean.
                            -- string must be "sigmoid" or "max_normalize"
            logvar_type: the type of log-variance estimator to use at output
                         -- single_shared: one value for all dimensions of all
                                           outputs
                         -- multi_shared: one value for each dimension of all
                                          outputs
                         -- single_estimate: one value for all dimensions of
                                             each  output
                         -- multi_estimate: one value for each dimension of
                                            each output
            activation: "function handle" for the desired non-linearity
            init_scale: scaling factor for hidden layer weights (__ * 0.01)
        shared_param_dicts: parameters for the MLP controlled by this GenNet
    """
    def __init__(self, \
            rng=None, \
            Xp=None, \
            prior_sigma=None, \
            params=None, \
            shared_param_dicts=None):
        # First, setup a shared random number generator for this layer
        self.rng = RandStream(rng.randint(1000000))

        # Grab the symbolic input matrix
        self.Xp = Xp
        self.prior_sigma = prior_sigma
        #####################################################
        # Process user-supplied parameters for this network #
        #####################################################
        assert(not (params is None))
        self.params = params
        lam_l2a = self.params['lam_l2a']
        if 'build_theano_funcs' in params:
            self.build_theano_funcs = params['build_theano_funcs']
        else:
            self.build_theano_funcs = True
        if 'vis_drop' in self.params:
            # Drop rate on the latent variables
            self.vis_drop = self.params['vis_drop']
        else:
            self.vis_drop = 0.0
        if 'hid_drop' in self.params:
            # Drop rate on hidden layer activations
            self.hid_drop = self.params['hid_drop']
        else:
            self.hid_drop = 0.0
        if 'bias_noise' in self.params:
            # Noise sigma for hidden layer biases
            self.bias_noise = self.params['bias_noise']
        else:
            self.bias_noise = 0.0
        if 'init_scale' in params:
            self.init_scale = params['init_scale']
        else:
            self.init_scale = 1.0
        if 'out_type' in params:
            # check which type of output distribution to generate
            self.out_type = params['out_type']
            assert((self.out_type == 'bernoulli') or \
                    (self.out_type == 'gaussian'))
        else:
            # default to bernoulli-valued outputs
            self.out_type = 'bernoulli'
        if 'mean_transform' in params:
            trans = params['mean_transform']
            assert((trans == 'sigmoid') or (trans == 'max_normalize'))
            if trans == 'sigmoid':
                self.mean_transform = lambda x: T.nnet.sigmoid(x)
            else:
                self.mean_transform = lambda x: max_normalize(x, axis=1)
        else:
            self.mean_transform = lambda x: x
        if 'logvar_type' in params:
            # single_shared: one bias shared between all output distros
            # multi_shared: per-dim biases shared between all output distros
            # single_estimate: one bias, estimated per output distro
            # multi_estimate: per-dim biases, estimated per output distro
            self.logvar_type = params['logvar_type']
            assert((self.logvar_type == 'single_shared') or \
                    (self.logvar_type == 'multi_shared') or \
                    (self.logvar_type == 'single_estimate') or \
                    (self.logvar_type == 'multi_estimate'))
        else:
            self.logvar_type = 'single_shared'
        # Get the configuration/prototype for this network. The config is a
        # list of layer descriptions, including a description for the input
        # layer, which is typically just the dimension of the inputs. So, the
        # depth of the mlp is one less than the number of layer configs.
        self.mlp_config = params['mlp_config']
        if 'activation' in params:
            self.activation = params['activation']
        else:
            self.activation = relu_actfun
        self.mlp_depth = len(self.mlp_config) - 1
        self.latent_dim = self.mlp_config[0]
        self.data_dim = self.mlp_config[-1]
        if 'decoder' in params:
            # we want to transform samples from the generator into some
            # other space in a convenient way. e.g., we want to construct the
            # generative model for some images in a dimension-reduced PCA space
            # rather than in the original pixel space. but, we want to be able
            # to sample easily from the model distribution in pixel space...
            self.encoder = params['encoder']
            self.decoder = params['decoder']
            self.use_decoder = True
        else:
            # no transform was given by the user, so we'll just apply a no-op
            self.encoder = lambda x: x
            self.decoder = lambda x: x
            self.use_decoder = False
        # Check if the params for this net were given a priori. This option
        # will be used for creating "clones" of a generative network, with all
        # of the network parameters shared between clones.
        if shared_param_dicts is None:
            # This is not a clone, and we will need to make a dict for
            # referring to the parameters of each network layer
            self.shared_param_dicts = []
            self.is_clone = False
        else:
            # This is a clone, and its layer parameters can be found by
            # referring to the given param dict (i.e. shared_param_dicts).
            self.shared_param_dicts = shared_param_dicts
            self.is_clone = True

        ##########################
        # Initialize the network #
        ##########################
        self.mlp_layers = []
        self.logvar_layer = None
        layer_def_pairs = zip(self.mlp_config[:-1],self.mlp_config[1:])
        layer_num = 0
        next_input = self.Xp
        for in_def, out_def in layer_def_pairs:
            first_layer = (layer_num == 0)
            last_layer = (layer_num == (len(layer_def_pairs) - 1))
            l_name = "gn_layer_{0:d}".format(layer_num)
            if (type(in_def) is list) or (type(in_def) is tuple):
                # Receiving input from a poolish layer...
                in_dim = in_def[0]
            else:
                # Receiving input from a normal layer...
                in_dim = in_def
            if (type(out_def) is list) or (type(out_def) is tuple):
                # Applying some sort of pooling in this layer...
                out_dim = out_def[0]
                pool_size = out_def[1]
            else:
                # Not applying any pooling in this layer...
                out_dim = out_def
                pool_size = 0
            # Select the appropriate noise to add to this layer
            if first_layer:
                d_rate = self.vis_drop
            else:
                d_rate = self.hid_drop
            # set in-bound weights to have norm self.init_scale
            i_scale = (1.0 / np.sqrt(in_dim)) * self.init_scale
            b_noise = self.bias_noise
            if not self.is_clone:
                ##########################################
                # Initialize a layer with new parameters #
                ##########################################
                new_layer = HiddenLayer(rng=rng, input=next_input, \
                        activation=self.activation, pool_size=pool_size, \
                        drop_rate=d_rate, input_noise=0., bias_noise=b_noise, \
                        in_dim=in_dim, out_dim=out_dim, \
                        name=l_name, W_scale=i_scale)
                self.mlp_layers.append(new_layer)
                self.shared_param_dicts.append( \
                        {'W': new_layer.W, 'b': new_layer.b})
                if (last_layer and (self.out_type != 'bernoulli')):
                    # add an extra layer/transform for encoding log-variance
                    lv_layer = HiddenLayer(rng=rng, input=next_input, \
                        activation=self.activation, pool_size=pool_size, \
                        drop_rate=d_rate, input_noise=0., bias_noise=b_noise, \
                        in_dim=in_dim, out_dim=out_dim, \
                        name=l_name+'_logvar', W_scale=0.1*i_scale)
                    self.logvar_layer = lv_layer
                    self.mlp_layers.append(lv_layer)
                    self.shared_param_dicts.append( \
                            {'W': lv_layer.W, 'b': lv_layer.b})
            else:
                ##################################################
                # Initialize a layer with some shared parameters #
                ##################################################
                init_params = self.shared_param_dicts[layer_num]
                self.mlp_layers.append(HiddenLayer(rng=rng, input=next_input, \
                        activation=self.activation, pool_size=pool_size, \
                        drop_rate=d_rate, input_noise=0., bias_noise=b_noise, \
                        in_dim=in_dim, out_dim=out_dim, \
                        W=init_params['W'], b=init_params['b'], \
                        name=l_name, W_scale=i_scale))
                if (last_layer and (self.out_type != 'bernoulli')):
                    # add an extra layer for predicting log-variance.
                    # we'll shrink the init scale for this layer, to avoid
                    # unreasonably small initial predicted logvars.
                    init_params = self.shared_param_dicts[layer_num+1]
                    self.mlp_layers.append(HiddenLayer(rng=rng, input=next_input, \
                        activation=self.activation, pool_size=pool_size, \
                        drop_rate=d_rate, input_noise=0., bias_noise=b_noise, \
                        in_dim=in_dim, out_dim=out_dim, \
                        W=init_params['W'], b=init_params['b'], \
                        name=l_name+'_logvar', W_scale=0.1*i_scale))
            next_input = self.mlp_layers[-1].output
            # Acknowledge layer completion
            layer_num = layer_num + 1

        # construct a mask for deciding which output dimensions to keep/ignore
        if self.is_clone:
            self.output_mask = self.shared_param_dicts[-1]['output_mask']
            self.output_bias = self.shared_param_dicts[-1]['output_bias']
            self.logvar_bias = self.shared_param_dicts[-1]['logvar_bias']
        else:
            row_mask = np.ones((self.data_dim,)).astype(theano.config.floatX)
            self.output_mask = theano.shared(value=row_mask, name='gn_output_mask')
            row_mask = 0.0 * row_mask
            self.output_bias = theano.shared(value=row_mask, name='gn_output_bias')
            row_mask = 0.0 * row_mask
            self.logvar_bias = theano.shared(value=row_mask, name='gn_logvar_bias')
            op_dict = {'output_mask': self.output_mask, \
                       'output_bias': self.output_bias, \
                       'logvar_bias': self.logvar_bias}
            self.shared_param_dicts.append(op_dict)

        # Mash all the parameters together, into a list.
        self.mlp_params = []
        for layer in self.mlp_layers:
            self.mlp_params.extend(layer.params)
        # add the output bias vector to the param list
        self.mlp_params.append(self.output_bias)
        if (self.out_type != 'bernoulli'):
            self.mlp_params.append(self.logvar_bias)

        # The output of this generator network is given by the noisy output
        # of its final layer. We will keep a running estimate of the mean and
        # covariance of the distribution induced by combining this network's
        # latent noise source with its deep non-linear transform. These will
        # be used to encourage the induced distribution to match the first and
        # second-order moments of the distribution we are trying to match.
        if self.out_type == 'bernoulli':
            raw_mean = self.mlp_layers[-1].linear_output + self.output_bias
            self.output = T.nnet.sigmoid(raw_mean)
            self.output_logvar = 0.0 * self.output
            self.output_sigma = 0.0 * self.output
        else:
            raw_mean = self.mlp_layers[-2].linear_output + self.output_bias
            # apply a transform (it's just identity if none was given)
            self.output = self.mean_transform(raw_mean)
            # set self.output_logvar based on self.logvar_type
            if self.logvar_type == 'single_shared':
                # one logvar, shared by all output distros
                self.output_logvar = 6.0 * (T.tanh(self.logvar_bias[0] / 6.0))
            elif self.logvar_type == 'multi_shared':
                # diagonal logvar, shared by all output distros
                self.output_logvar = 6.0 * (T.tanh(self.logvar_bias / 6.0))
            elif self.logvar_type == 'single_estimate':
                # one logvar, estimated per output distro
                self.output_logvar = 6.0 * \
                        T.tanh(T.mean(self.mlp_layers[-1].linear_output) / 6.0)
            else: # here, self.logvar_type == 'multi_estimate'
                # diagonal logvar, estimated per output distro
                self.output_logvar = 6.0 * \
                        T.tanh(self.mlp_layers[-1].linear_output / 6.0)
            self.output_sigma = T.exp(0.5 * self.output_logvar)
            self.output_samples = self._construct_post_samples()
        # apply a decoder (or a no-op if the user didn't provide a decoder)
        if self.use_decoder:
            self.output_decoded = self.decoder(self.output)
        else:
            self.output_decoded = self.output
        self.out_dim = self.mlp_layers[-1].out_dim
        C_init = np.zeros((self.out_dim,self.out_dim)).astype(theano.config.floatX)
        m_init = np.zeros((self.out_dim,)).astype(theano.config.floatX)
        self.dist_mean = theano.shared(m_init, name='gn_dist_mean')
        self.dist_cov = theano.shared(C_init, name='gn_dist_cov')
        # Get simple regularization penalty to moderate activation dynamics
        self.act_reg_cost = lam_l2a * self._act_reg_cost()
        # Construct a sampler for drawing independent samples from this model's
        # isotropic Gaussian prior, and a sampler for the model distribution.
        if self.build_theano_funcs:
            self.sample_from_prior = self._construct_prior_sampler()
            self.sample_from_model = self._construct_model_sampler()
            self.scaled_sampler = self._construct_scaled_sampler()
            # Construct a function for passing points from the latent/prior space
            # through the transform induced by the current model parameters.
            self.transform_prior = self._construct_transform_prior()
        else:
            self.sample_from_prior = None
            self.sample_from_model = None
            self.scaled_sampler = None
            self.transform_prior = None

        ########################################################
        # CONSTRUCT FUNCTIONS FOR RICA PRETRAINING OUTPUT MEAN #
        ########################################################
        self.rica_func = None
        if ('gaussian' in self.out_type):
            self.W_rica = self.mlp_layers[-2].W
        else:
            self.W_rica = self.mlp_layers[-1].W
        return

    def train_rica(self, X, lr, lam):
        """
        CONSTRUCT FUNCTIONS FOR RICA PRETRAINING OUTPUT MEAN
        """
        if self.rica_func is None:
            l_rate = T.scalar()
            lam_l1 = T.scalar()
            W_out = self.W_rica + self.rng.normal(size=self.W_rica.shape, \
                avg=0.0, std=0.01, dtype=theano.config.floatX)
            X_out = T.matrix('gn_X_out')
            X_enc = self.encoder(X_out)
            H_rec = T.dot(X_enc, W_out.T)
            X_rec = T.dot(H_rec, W_out)
            recon_cost = T.sum((X_enc - X_rec)**2.0) / X_enc.shape[0]
            spars_cost = lam_l1 * (T.sum(soft_abs(H_rec)) / H_rec.shape[0])
            rica_cost = recon_cost + spars_cost
            dW = T.grad(rica_cost, self.W_rica)
            rica_updates = {self.W_rica: self.W_rica - (l_rate * dW)}
            rica_outputs = [rica_cost, recon_cost, spars_cost]
            self.rica_func = theano.function([X_out, l_rate, lam_l1], \
                    outputs=rica_outputs, \
                    updates=rica_updates)
        outputs = self.rica_func(X, lr, lam)
        return outputs

    def _act_reg_cost(self):
        """
        Apply L2 regularization to the activations in this network.
        """
        act_sq_sums = []
        for layer in self.mlp_layers:
            act_sq_sums.append(layer.act_l2_sum)
        full_act_sq_sum = T.sum(act_sq_sums)
        return full_act_sq_sum

    def _construct_post_samples(self):
        """
        Draw a single sample from each of the approximate posteriors encoded
        in self.output (e.g. the Gaussian's mean) and self.output_sigma.
        """
        post_samples = self.output + (self.output_sigma * \
                self.rng.normal(size=self.output.shape, avg=0.0, std=1.0, \
                dtype=theano.config.floatX))
        return post_samples

    def _construct_prior_sampler(self):
        """
        Draw independent samples from this model's isotropic Gaussian prior.
        """
        samp_count = T.lscalar()
        prior_samples = self.prior_sigma * self.rng.normal( \
                size=(samp_count, self.latent_dim), avg=0.0, std=1.0, \
                dtype=theano.config.floatX)
        prior_sampler = theano.function([samp_count], outputs=prior_samples)
        return prior_sampler

    def _construct_model_sampler(self):
        """
        Draw independent samples from this model's distribution.
        """
        samp_count = T.lscalar()
        prior_samples = self.prior_sigma * self.rng.normal( \
                size=(samp_count, self.latent_dim), avg=0.0, std=1.0, \
                dtype=theano.config.floatX)
        if self.use_decoder:
            prior_sampler = theano.function([samp_count], \
                    outputs=self.output_decoded, \
                    givens={self.Xp: prior_samples})
        else:
            prior_sampler = theano.function([samp_count], outputs=self.output, \
                    givens={self.Xp: prior_samples})
        return prior_sampler

    def _construct_scaled_sampler(self):
        """
        Draw independent samples from this model's distribution. But, but,
        but, draw latent samples with a user-defined standard deviation!
        """
        samp_count = T.lscalar()
        user_sigma = T.scalar()
        prior_samples = user_sigma * self.rng.normal( \
                size=(samp_count, self.latent_dim), avg=0.0, std=1.0, \
                dtype=theano.config.floatX)
        if self.use_decoder:
            prior_sampler = theano.function([samp_count, user_sigma], \
                    outputs=self.output_decoded, \
                    givens={self.Xp: prior_samples})
        else:
            prior_sampler = theano.function([samp_count, user_sigma], \
                    outputs=self.output, \
                    givens={self.Xp: prior_samples})
        return prior_sampler

    def _construct_transform_prior(self):
        """
        Apply the tranform induced by the current model parameters to some
        set of points in the latent/prior space.
        """
        if self.use_decoder:
            # transform the output of this generator, presumably to match the
            # expected input shape for an associated inferencer (for looping)
            feedforward = theano.function([self.Xp], \
                    outputs=self.output_decoded)
        else:
            # take the "encoded" output of the generator because, whatever
            # inferencer we might feed this back into, it's the right shape
            feedforward = theano.function([self.Xp], outputs=self.output)
        return feedforward

    def _batch_moments(self):
        """
        Compute covariance and mean of the current sample outputs.
        """
        mu = T.mean(self.output, axis=0, keepdims=True)
        sigma = T.dot((self.output.T - mu.T), (self.output - mu))
        return [mu, sigma]

    def init_biases(self, b_init=0.0, b_std=1e-2):
        """
        Initialize the biases in all hidden layers to some constant.
        """
        if (self.out_type == 'bernoulli'):
            # Always start with 0 bias on the pre-sigmoid output
            for layer in self.mlp_layers[:-1]:
                b_vec = (0.0 * layer.b.get_value(borrow=False)) + b_init
                b_vec = b_vec + (b_std * npr.randn(*b_vec.shape))
                layer.b.set_value(b_vec.astype(theano.config.floatX))
        else:
            # Always start with 0 bias on the output mean and log-variance
            for layer in self.mlp_layers[:-2]:
                b_vec = (0.0 * layer.b.get_value(borrow=False)) + b_init
                b_vec = b_vec + (b_std * npr.randn(*b_vec.shape))
                layer.b.set_value(b_vec.astype(theano.config.floatX))
        return

    def init_moments(self, X_noise):
        """
        Initialize the running mean and covariance estimates.
        """
        X_noise_sym = T.matrix()
        out_func = theano.function(inputs=[ X_noise_sym ], \
                outputs=[ self.output ], \
                givens={self.Xp: X_noise_sym})
        # Compute outputs for the input latent noise matrix
        X_out = out_func(X_noise.astype(theano.config.floatX))[0]
        # Compute mean and covariance of the outputs
        mu = np.mean(X_out, axis=0)
        X_out_minus_mu = X_out - mu
        sigma = np.dot(X_out_minus_mu.T,X_out_minus_mu) / X_out.shape[0]
        # Initialize the network's running estimates 
        self.dist_cov.set_value(sigma.astype(theano.config.floatX))
        self.dist_mean.set_value(mu.astype(theano.config.floatX))
        return

    def set_output_mask(self, output_mask):
        """
        Set a (probably) binary mask on the output dimensions.
        """
        print("MASKING IS NON-FUNCTIONAL FOR NOW")
        assert(False and (output_mask.size == self.data_dim))
        output_mask = output_mask.reshape((self.data_dim,))
        self.output_mask.set_value(output_mask.astype(theano.config.floatX))
        return

    def compute_log_prob(self, Xd=None):
        """
        Compute negative log likelihood of the data in Xd, with respect to the
        output distributions currently at self.output....

        Compute log-prob for all entries in Xd.
        """
        if (self.out_type == 'bernoulli'):
            log_prob_cost = log_prob_bernoulli(Xd, self.output, mask=self.output_mask)
        else:
            log_prob_cost = log_prob_gaussian2(Xd, self.output, \
                    log_vars=self.output_logvar, mask=self.output_mask)
        return log_prob_cost

    def masked_log_prob(self, Xc=None, Xm=None):
        """
        Compute negative log likelihood of the data in Xc, with respect to the
        output distributions currently at self.output....

        Select entries in Xd to compute log-prob for based on the mask Xm. When
        Xm[i] == 1, don't measure NLL Xc[i]...
        """
        # to measure NLL for Xc[i] only when Xm[i] is 0, we need to make an
        # inverse mask Xm_inv = 1 - X_m, because the masking in the log pdf
        # functions measures NLL only for observations where the mask != 0.
        Xm_inv = 1.0 - Xm
        if (self.out_type == 'bernoulli'):
            log_prob_cost = log_prob_bernoulli(Xc, self.output, mask=Xm_inv)
        else:
            log_prob_cost = log_prob_gaussian2(Xc, self.output, \
                    log_vars=self.output_logvar, mask=Xm_inv)
        return log_prob_cost

    def shared_param_clone(self, rng=None, Xp=None):
        """
        Return a clone of this network, with shared parameters but with
        different symbolic input variables.

        This can be used for "unrolling" a generate->infer->generate->infer...
        loop. Then, we can do backprop through time for various objectives.
        """
        clone_net = GenNet(rng=rng, Xp=Xp, \
                prior_sigma=self.prior_sigma, params=self.params, \
                shared_param_dicts=self.shared_param_dicts)
        return clone_net

    def save_to_file(self, f_name=None):
        """
        Dump important stuff to a Python pickle, so that we can reload this
        model later. We'll pickle everything required to create a clone of
        this model given the pickle and the rng/Xd params to the cloning
        function: "GenNet.shared_param_clone()".
        """
        assert(not (f_name is None))
        f_handle = file(f_name, 'wb')
        # dump the "simple" python value in self.prior_sigma
        cPickle.dump(self.prior_sigma, f_handle, protocol=-1)
        # dump the dict self.params, which just holds "simple" python values
        cPickle.dump(self.params, f_handle, protocol=-1)
        # make a copy of self.shared_param_dicts, with numpy arrays in place
        # of the theano shared variables
        numpy_param_dicts = []
        for shared_dict in self.shared_param_dicts:
            numpy_dict = {}
            for key in shared_dict:
                numpy_dict[key] = shared_dict[key].get_value(borrow=False)
            numpy_param_dicts.append(numpy_dict)
        # dump the numpy version of self.shared_param_dicts
        cPickle.dump(numpy_param_dicts, f_handle, protocol=-1)
        f_handle.close()
        return

def load_gennet_from_file(f_name=None, rng=None, Xp=None, new_params=None):
    """
    Load a clone of some previously trained model. Allow the drop rates in
    the loaded model to be clamped at 0 post-hoc.
    """
    assert(not (f_name is None))
    pickle_file = open(f_name)
    self_dot_prior_sigma = cPickle.load(pickle_file)
    self_dot_params = cPickle.load(pickle_file)
    if not (new_params is None):
        for k in new_params:
            self_dot_params[k] = new_params[k]
    self_dot_numpy_param_dicts = cPickle.load(pickle_file)
    self_dot_shared_param_dicts = []
    for numpy_dict in self_dot_numpy_param_dicts:
        shared_dict = {}
        for key in numpy_dict:
            val = numpy_dict[key].astype(theano.config.floatX)
            shared_dict[key] = theano.shared(val)
        self_dot_shared_param_dicts.append(shared_dict)
    # now, create a PeaNet with the configuration we just unpickled
    clone_net = GenNet(rng=rng, Xp=Xp, \
            prior_sigma=self_dot_prior_sigma, params=self_dot_params, \
            shared_param_dicts=self_dot_shared_param_dicts)
    # helpful output
    print("==================================================")
    print("LOADED GenNet WITH PARAMS:")
    for k in self_dot_params:
        print("    {0:s}: {1:s}".format(str(k), str(self_dot_params[k])))
    print("==================================================")
    return clone_net


if __name__=="__main__":
    # Derp
    print("NO TEST/DEMO CODE FOR NOW.")
