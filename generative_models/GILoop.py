###################################################################
# Code for managing and training a generator/discriminator pair.  #
###################################################################

# basic python
import numpy as np
import numpy.random as npr
from collections import OrderedDict

# theano business
import theano
import theano.tensor as T
from theano.ifelse import ifelse
import theano.tensor.shared_randomstreams
from theano.sandbox.cuda.rng_curand import CURAND_RandomStreams

# phil's sweetness
from NetLayers import HiddenLayer, DiscLayer, relu_actfun, softplus_actfun
from GenNet import GenNet, projected_moments
from InfNet import InfNet
from EarNet import EarNet


#
# The G<->I loop is constructed via loop unrolling. Note that the unrolled
# loop is built from "shared-parameter clones" of a base generator and
# inferencer that are provided as parameters when the unrolled loop is
# constructed. A clone of the base generator network appears first in the
# unrolled loop. A clone of the base inferencer network appears last. The
# various operating modes for the unrolled loop are effected by injecting input
# to the loop through the "prior variables" of the first generator clone, or
# through the "data variables" of the first inferencer clone, or through the
# "control variables", "mask variables", and "data variables" of the first
# inferencer clone. Respectively, we'll refer to these operating modes as:
# "prior-driven", "data-driven", and "control-driven".
#
# Important symbolic variables:
#   Xp: Xp represents input at the "prior variables" of the first generator
#       network clone.
#   Xd: Xd represents input at the "data variables" of the first inferencer
#       network clone.
#   Xc: Xc represents input at the "control variables" of all of the inferencer
#       network clones.
#   Xm: Xm represents input at the "mask variables" of all of the inferencer
#       network clones.
#
# When operating in "prior-driven" mode, only values for Xp will be required as
# exogenous input. The values of Xc and Xm for all inferencer clones will be
# set to 0. The input to the first inferencer clone (which might otherwise be
# set via Xd) will be given by the output of the first generator clone.
#
# When operating in "data-driven" mode, only values for Xd will be required as
# exogenous input. The first generator clone won't need to compute anything
# because the "data variables" of the first inferencer clone will be set via
# Xd. Xc and Xm for all inferencer clones will be set to 0.
#
# When operating in "control-driven" mode, values for Xd, Xc, and Xm will all
# be required as exogenous input. The first generator clone won't need to
# compute anything because the all inputs (i.e. data, control, and mask) for
# the first inferencer clone will be set via Xd, Xc, and Xm.
#
#

class GILoop(object):
    """
    Controller for propagating through a generate<->inference loop.

    The generator must be an instance of the GenNet class implemented in
    "GINets.py". The discriminator must be an instance of the EarNet class,
    as implemented in "EarNet.py".

    Parameters:
        rng: numpy.random.RandomState (for reproducibility)
        g_net: The GenNet instance that will serve as the base generator
        i_net: The InfNet instance that will serve as the base inferer
        data_dim: dimension of the "observable data" variables
        prior_dim: dimension of the "latent prior" variables
        loop_iters: The number of loop cycles to unroll
        params: a dict of parameters for controlling various costs
            mom_mix_rate: rate for updates to the running moment estimates
                          for the distribution generated by g_net
            mom_match_weight: weight for the "moment matching" cost
            mom_match_proj: projection matrix for reduced-dim mom matching
            target_mean: first-order moment to try and match with g_net
            target_cov: second-order moment to try and match with g_net
    """
    def __init__(self, rng=None, g_net=None, i_net=None, data_dim=None, \
            prior_dim=None, loop_iters=1, params=None):
        # Do some stuff!
        self.rng = theano.tensor.shared_randomstreams.RandomStreams( \
                rng.randint(100000))
        self.GN = g_net
        self.IN = i_net
        self.data_dim = data_dim
        self.prior_dim = prior_dim
        self.loop_iters = loop_iters

        # output of the generator and input to the inferencer should both be
        # equal to self.data_dim
        assert(self.data_dim == self.GN.mlp_layers[-1].out_dim)
        assert(self.data_dim == self.IN.shared_layers[0].in_dim)
        # input of the generator and mu/sigma outputs of the inferencer should
        # both be equal to self.prior_dim
        assert(self.prior_dim == self.GN.mlp_layers[0].in_dim)
        assert(self.prior_dim == self.IN.mu_layers[-1].out_dim)
        assert(self.prior_dim == self.IN.sigma_layers[-1].out_dim)

        # create symbolic vars for feeding inputs to the generator/inferencer
        self.Xp = T.matrix(name='gil_Xp')
        self.Xd = T.matrix(name='gil_Xd')
        self.Xc = T.matrix(name='gil_Xc')
        self.Xm = T.matrix(name='gil_Xm')

        # shared var learning rate for generator and inferencer
        zero_ary = np.zeros((1,)).astype(theano.config.floatX)
        self.lr_gn = theano.shared(value=zero_ary, name='gil_lr_gn')
        self.lr_in = theano.shared(value=zero_ary, name='gil_lr_in')
        # shared var momentum parameters for generator and inferencer
        self.mo_gn = theano.shared(value=zero_ary, name='gil_mo_gn')
        self.mo_in = theano.shared(value=zero_ary, name='gil_mo_in')
        # init parameters for controlling learning dynamics
        self.set_gn_sgd_params() # init SGD rate/momentum for GN
        self.set_in_sgd_params() # init SGD rate/momentum for IN

        #######################################################
        # Welcome to: Moment Matching Cost Information Center #
        #######################################################
        #
        # Get parameters for managing the moment matching cost. The moment
        # matching is based on exponentially-decaying estimates of the mean
        # and covariance of the distribution induced by the generator network
        # and the (latent) noise being fed to it.
        #
        # We provide the option of performing moment matching with either the
        # raw generator output, or with linearly-transformed generator output.
        # Either way, the given target mean and covariance should have the
        # appropriate dimension for the space in which we'll be matching the
        # generator's 1st/2nd moments with the target's 1st/2nd moments. For
        # clarity, the computation we'll perform looks like:
        #
        #   Xm = X - np.mean(X, axis=0)
        #   XmP = np.dot(Xm, P)
        #   C = np.dot(XmP.T, XmP)
        #
        # where Xm is the mean-centered samples from the generator and P is
        # the matrix for the linear transform to apply prior to computing
        # the moment matching cost. For simplicity, the above code ignores the
        # use of an exponentially decaying average to track the estimated mean
        # and covariance of the generator's output distribution.
        #
        # The relative contribution of the current batch to these running
        # estimates is determined by self.mom_mix_rate. The mean estimate is
        # first updated based on the current batch, then the current batch
        # is centered with the updated mean, then the covariance estimate is
        # updated with the mean-centered samples in the current batch.
        #
        # Strength of the moment matching cost is given by self.mom_match_cost.
        # Target mean/covariance are given by self.target_mean/self.target_cov.
        # If a linear transform is to be applied prior to matching, it is given
        # by self.mom_match_proj.
        #
        zero_ary = np.zeros((1,))
        mmr = zero_ary + params['mom_mix_rate']
        self.mom_mix_rate = theano.shared(name='gil_mom_mix_rate', \
            value=mmr.astype(theano.config.floatX))
        mmw = zero_ary + params['mom_match_weight']
        self.mom_match_weight = theano.shared(name='gil_mom_match_weight', \
            value=mmw.astype(theano.config.floatX))
        targ_mean = params['target_mean'].astype(theano.config.floatX)
        targ_cov = params['target_cov'].astype(theano.config.floatX)
        assert(targ_mean.size == targ_cov.shape[0]) # mean and cov use same dim
        assert(targ_cov.shape[0] == targ_cov.shape[1]) # cov must be square
        self.target_mean = theano.shared(value=targ_mean, name='gil_target_mean')
        self.target_cov = theano.shared(value=targ_cov, name='gil_target_cov')
        mmp = np.identity(targ_cov.shape[0]) # default to identity transform
        if 'mom_match_proj' in params:
            mmp = params['mom_match_proj'] # use a user-specified transform
        assert(mmp.shape[0] == self.data_dim) # transform matches data dim
        assert(mmp.shape[1] == targ_cov.shape[0]) # and matches mean/cov dims
        mmp = mmp.astype(theano.config.floatX)
        self.mom_match_proj = theano.shared(value=mmp, name='gil_mom_map_proj')
        # finally, we can construct the moment matching cost! and the updates
        # for the running mean/covariance estimates too!
        self.mom_match_cost, self.mom_updates = self._construct_mom_stuff()
        #########################################
        # Thank you for visiting the M.M.C.I.C. #
        #########################################

        ###############################################
        # construct the G<->I loop via loop unrolling #
        #                                             ######################### 
        # we will actually construct two unrolled loops, one for when we want #
        # to start with the generator (i.e. sample from prior), and one for   #
        # when we want to start with the inferencer (i.e. data sample and/or  #
        # some control input). starting with the generator requires Xp as an  #
        # input, and starting the inferencer requires Xd, Xc, and Xm.         #
        #######################################################################
        self.gil_prior = []
        self.gil_data = []
        for i in range(self.loop_iters):
            # construct gi_pair for the prior-driven loop
            gi_pair = {'GN': None, 'IN': None}
            if (i == 0):
                # the first generator receives exogenous input and feeds its
                # output into the first inferencer
                g_n = self.GN.shared_param_clone(rng=rng, Xp=self.Xp)
                i_n = self.IN.shared_param_clone(rng=rng, Xd=g_n.output, \
                        Xc=self.Xc, Xm=self.Xm)
                gi_pair['GN'] = g_n
                gi_pair['IN'] = i_n
            else:
                # the remaining generators receive their inputs from an
                # inferencer in the previous loop cycle, and feed their outputs
                # into the inferencer in their loop cycle
                prev_in = self.gil_prior[-1]['IN']
                g_n = self.GN.shared_param_clone(rng=rng, Xp=prev_in.output)
                i_n = self.IN.shared_param_clone(rng=rng, Xd=g_n.output, \
                        Xc=self.Xc, Xm=self.Xm)
                gi_pair['GN'] = g_n
                gi_pair['IN'] = i_n
            self.gil_prior.append(gi_pair)
            # construct gi_pair for the data-driven loop
            gi_pair = {'GN': None, 'IN': None}
            if (i == 0):
                # the first inferencer receives exogenous input and feeds its
                # output into the first generator
                i_n = self.IN.shared_param_clone(rng=rng, Xd=self.Xd, \
                        Xc=self.Xc, Xm=self.Xm)
                g_n = self.GN.shared_param_clone(rng=rng, Xp=i_n.output)
                gi_pair['IN'] = i_n
                gi_pair['GN'] = g_n
            else:
                # the remaining inferencers receive their inputs from a
                # generator in the previous loop cycle, and feed their outputs
                # into the generator in their loop cycle
                prev_gn = self.gil_data[-1]['GN']
                i_n = self.IN.shared_param_clone(rng=rng, Xd=prev_gn.output, \
                        Xc=self.Xc, Xm=self.Xm)
                g_n = self.GN.shared_param_clone(rng=rng, Xp=i_n.output)
                gi_pair['IN'] = i_n
                gi_pair['GN'] = g_n
            self.gil_data.append(gi_pair)

        # Grab the full set of "optimizable" parameters from the generator
        # and inferencer networks that we'll be working with.
        self.in_params = [p for p in self.IN.mlp_params]
        self.gn_params = [p for p in self.GN.mlp_params]

        # TODO: construct the cost functions for gen <-> inf loop
        self.in_cost = self.IN.act_reg_cost
        self.gn_cost = self.GN.act_reg_cost

        # Initialize momentums for mini-batch SGD updates. All parameters need
        # to be safely nestled in their lists by now.
        self.joint_moms = OrderedDict()
        self.in_moms = OrderedDict()
        self.gn_moms = OrderedDict()
        for p in self.gn_params:
            p_mo = np.zeros(p.get_value(borrow=True).shape)
            self.gn_moms[p] = theano.shared(value=p_mo.astype(theano.config.floatX))
            self.joint_moms[p] = self.gn_moms[p]
        for p in self.in_params:
            p_mo = np.zeros(p.get_value(borrow=True).shape)
            self.in_moms[p] = theano.shared(value=p_mo.astype(theano.config.floatX))
            self.joint_moms[p] = self.in_moms[p]

        # Construct the updates for the generator and inferer networks
        self.joint_updates = OrderedDict()
        self.gn_updates = OrderedDict()
        self.in_updates = OrderedDict()
        for var in self.in_params:
            # these updates are for trainable params in the inferencer net...
            # first, get gradient of cost w.r.t. var
            var_grad = T.grad(self.in_cost, var, \
                    consider_constant=[self.GN.dist_mean, self.GN.dist_cov])
            # get the momentum for this var
            var_mom = self.in_moms[var]
            # update the momentum for this var using its grad
            self.in_updates[var_mom] = (self.mo_in[0] * var_mom) + \
                    ((1.0 - self.mo_in[0]) * var_grad)
            self.joint_updates[var_mom] = self.in_updates[var_mom]
            # make basic update to the var
            var_new = var - (self.lr_in[0] * var_mom)
            if ((var in self.IN.clip_params) and \
                    (var in self.IN.clip_norms) and \
                    (self.IN.clip_params[var] == 1)):
                # clip the basic updated var if it is set as clippable
                clip_norm = self.IN.clip_norms[var]
                var_norms = T.sum(var_new**2.0, axis=1, keepdims=True)
                var_scale = T.clip(T.sqrt(clip_norm / var_norms), 0., 1.)
                self.in_updates[var] = var_new * var_scale
            else:
                # otherwise, just use the basic updated var
                self.in_updates[var] = var_new
            # add this var's update to the joint updates too
            self.joint_updates[var] = self.in_updates[var]
        for var in self.mom_updates:
            # these updates are for the generator distribution's running first
            # and second-order moment estimates
            self.gn_updates[var] = self.mom_updates[var]
            self.joint_updates[var] = self.mom_updates[var]
        for var in self.gn_params:
            # these updates are for trainable params in the generator net...
            # first, get gradient of cost w.r.t. var
            var_grad = T.grad(self.gn_cost, var, \
                    consider_constant=[self.GN.dist_mean, self.GN.dist_cov])
            # get the momentum for this var
            var_mom = self.gn_moms[var]
            # update the momentum for this var using its grad
            self.gn_updates[var_mom] = (self.mo_gn[0] * var_mom) + \
                    ((1.0 - self.mo_gn[0]) * var_grad)
            self.joint_updates[var_mom] = self.gn_updates[var_mom]
            # make basic update to the var
            var_new = var - (self.lr_gn[0] * var_mom)
            if ((var in self.GN.clip_params) and \
                    (var in self.GN.clip_norms) and \
                    (self.GN.clip_params[var] == 1)):
                # clip the basic updated var if it is set as clippable
                clip_norm = self.GN.clip_norms[var]
                var_norms = T.sum(var_new**2.0, axis=1, keepdims=True)
                var_scale = T.clip(T.sqrt(clip_norm / var_norms), 0., 1.)
                self.gn_updates[var] = var_new * var_scale
            else:
                # otherwise, just use the basic updated var
                self.gn_updates[var] = var_new
            # add this var's update to the joint updates too
            self.joint_updates[var] = self.gn_updates[var]



        # Construct batch-based training functions for the generator and
        # inferer networks, as well as a joint training function.
        #self.train_gn = self._construct_train_gn()
        #self.train_in = self._construct_train_in()
        #self.train_joint = self._construct_train_joint()

        # Construct a function for computing the outputs of the generator
        # network for a batch of noise. Presumably, the noise will be drawn
        # from the same distribution that was used in training....
        self.sample_from_gn = self.GN.sample_from_model
        return

    def set_gn_sgd_params(self, learn_rate=0.02, momentum=0.9):
        """
        Set learning rate and momentum parameter for generator updates.
        """
        zero_ary = np.zeros((1,))
        new_lr = zero_ary + learn_rate
        self.lr_gn.set_value(new_lr.astype(theano.config.floatX))
        new_mo = zero_ary + momentum
        self.mo_gn.set_value(new_mo.astype(theano.config.floatX))
        return

    def set_in_sgd_params(self, learn_rate=0.02, momentum=0.9):
        """
        Set learning rate and momentum parameter for discriminator updates.
        """
        zero_ary = np.zeros((1,))
        new_lr = zero_ary + learn_rate
        self.lr_in.set_value(new_lr.astype(theano.config.floatX))
        new_mo = zero_ary + momentum
        self.mo_in.set_value(new_mo.astype(theano.config.floatX))
        return

    def _construct_mom_stuff(self):
        """
        Construct the cost function for the moment-matching "regularizer".
        """
        a = self.mom_mix_rate
        dist_mean = self.GN.dist_mean
        dist_cov = self.GN.dist_cov
        # Get the generated sample observations for this batch, transformed
        # linearly into the desired space for moment matching...
        X_b = T.dot(self.GN.output, self.mom_match_proj)
        # Get their mean
        batch_mean = T.mean(X_b, axis=0)
        # Get the updated generator distribution mean
        new_mean = ((1.0 - a[0]) * self.GN.dist_mean) + (a[0] * batch_mean)
        # Use the mean to get the updated generator distribution covariance
        X_b_minus_mean = X_b - new_mean
        # Whelp, I guess this line needs the cast... for some reason...
        batch_cov = T.dot(X_b_minus_mean.T, X_b_minus_mean) / T.cast(X_b.shape[0], 'floatX')
        new_cov = ((1.0 - a[0]) * self.GN.dist_cov) + (a[0] * batch_cov)
        # Get the cost for deviation from the target distribution's moments
        mean_err = new_mean - self.target_mean
        cov_err = (new_cov - self.target_cov)
        mm_cost = self.mom_match_weight[0] * \
                (T.sum(mean_err**2.0) + T.sum(cov_err**2.0))
        # Construct the updates for the running estimates of the generator
        # distribution's first and second-order moments.
        mom_updates = OrderedDict()
        mom_updates[self.GN.dist_mean] = new_mean
        mom_updates[self.GN.dist_cov] = new_cov
        return [mm_cost, mom_updates]

    def init_moments(self, sample_count):
        """
        Initialize estimates of the generator distribution's 1st and 2nd-order
        moments based on some large sample of input noise to the generator
        network. Estimates will be performed, and subsequently tracked, in a
        transformed space based on self.mom_match_proj.
        """
        # Compute outputs for the input latent noise in X_noise
        X = self.GN.sample_from_model(sample_count)
        # Get the transform to apply prior to moment matching
        P = self.mom_match_proj.get_value(borrow=False)
        # Compute post-transform mean and covariance of the outputs
        mu, sigma = projected_moments(X, P, ary_type='numpy')
        # Initialize the generator network's running moment estimates 
        self.GN.dist_cov.set_value(sigma.astype(theano.config.floatX))
        self.GN.dist_mean.set_value(mu.astype(theano.config.floatX))
        return

    def sample_gil_from_data(self, X_d, loop_iters=5):
        """
        Sample for several rounds through the G<->I loop, initialized with the
        the "data variable" samples in X_d.
        """
        data_samples = []
        prior_samples = []
        X_c = 0.0 * X_d
        X_m = 0.0 * X_d
        for i in range(loop_iters):
            # record the data samples for this iteration
            data_samples.append(1.0 * X_d)
            # sample from their inferred posteriors
            X_p = self.IN.sample_posterior(X_d, X_c, X_m)
            # record the sampled points (in the "prior space")
            prior_samples.append(1.0 * X_p)
            # get next data samples by transforming the prior-space points
            X_d = self.GN.transform_prior(X_p)
        result = {"data samples": data_samples, "prior samples": prior_samples}
        return result

    def sample_from_gil_prior(self):
        """
        TEMPORARY TEST FUNC
        """
        sample_count = 1000
        X_p = self.GN.sample_from_prior(sample_count)
        X_c = np.zeros((sample_count, self.data_dim)).astype(theano.config.floatX)
        X_m = np.zeros((sample_count, self.data_dim)).astype(theano.config.floatX)
        gi_pair = self.gil_prior[-1]
        gip_in = gi_pair['IN']
        sampler = theano.function([self.Xp, self.Xc, self.Xm], outputs=gip_in.output)
        sample_output = sampler(X_p, X_c, X_m)
        return sample_output

    def sample_from_gil_data(self):
        """
        TEMPORARY TEST FUNC
        """
        sample_count = 1000
        X_d = self.GN.sample_from_model(sample_count)
        X_c = np.zeros((sample_count, self.data_dim)).astype(theano.config.floatX)
        X_m = np.zeros((sample_count, self.data_dim)).astype(theano.config.floatX)
        gi_pair = self.gil_data[-1]
        gip_gn = gi_pair['GN']
        sampler = theano.function([self.Xd, self.Xc, self.Xm], outputs=gip_gn.output)
        sample_output = sampler(X_d, X_c, X_m)
        return sample_output

if __name__=="__main__":
    from load_data import load_udm, load_udm_ss, load_mnist
    
    # Initialize a source of randomness
    rng = np.random.RandomState(1234)

    # Load some data to train/validate/test with
    dataset = 'data/mnist.pkl.gz'
    datasets = load_udm(dataset, zero_mean=False)
    Xtr = datasets[0][0]
    tr_samples = Xtr.get_value(borrow=True).shape[0]
    data_dim = Xtr.get_value(borrow=True).shape[1]
    mm_proj_dim = 250
    # Do moment matching in some transformed space
    #P = np.identity(data_dim)
    P = npr.randn(data_dim, mm_proj_dim) / np.sqrt(float(mm_proj_dim))
    P = theano.shared(value=P.astype(theano.config.floatX), name='P_proj')
    target_mean, target_cov = projected_moments(Xtr, P, ary_type='theano')
    P = P.get_value(borrow=False).astype(theano.config.floatX)
    # Construct a GenNet and an InfNet, then test constructor for GILoop.
    # Do basic testing, to make sure classes aren't completely broken.
    Xp = T.matrix('Xp_base')
    Xd = T.matrix('Xd_base')
    Xc = T.matrix('Xc_base')
    Xm = T.matrix('Xm_base')
    data_dim = 28*28
    prior_dim = 100
    # Choose some parameters for the generator network
    gn_params = {}
    gn_config = [prior_dim, 500, 500, data_dim]
    gn_params['mlp_config'] = gn_config
    gn_params['activation'] = softplus_actfun
    gn_params['lam_l2a'] = 1e-3
    gn_params['vis_drop'] = 0.0
    gn_params['hid_drop'] = 0.0
    gn_params['bias_noise'] = 0.0
    gn_params['out_noise'] = 0.0
    # Choose some parameters for the inference network
    in_params = {}
    shared_config = [data_dim, 500]
    top_config = [shared_config[-1], 500, prior_dim]
    mu_config = top_config
    sigma_config = top_config
    in_params['shared_config'] = shared_config
    in_params['mu_config'] = mu_config
    in_params['sigma_config'] = sigma_config
    in_params['activation'] = softplus_actfun
    in_params['lam_l2a'] = 1e-3
    in_params['vis_drop'] = 0.0
    in_params['hid_drop'] = 0.0
    in_params['bias_noise'] = 0.0
    in_params['input_noise'] = 0.0
    # Initialize the base networks for this GILoop
    IN = InfNet(rng=rng, Xd=Xd, Xc=Xc, Xm=Xm, prior_sigma=5.0, \
            params=in_params, mlp_param_dicts=None)
    GN = GenNet(rng=rng, Xp=Xp, prior_sigma=5.0, \
            params=gn_params, mlp_param_dicts=None)
    # Setup parameters for the GILoop
    gil_params = {}
    gil_params['mom_mix_rate'] = 0.03
    gil_params['mom_match_weight'] = 0.05
    gil_params['mom_match_proj'] = P
    gil_params['target_mean'] = target_mean
    gil_params['target_cov'] = target_cov
    # Initialize the GILoop
    GIL = GILoop(rng=rng, g_net=GN, i_net=IN, data_dim=data_dim, \
            prior_dim=prior_dim, loop_iters=5, params=gil_params)
    GIL.init_moments(10000)

    print("TESTING COMPLETE!")




##############
# EYE BUFFER #
##############
